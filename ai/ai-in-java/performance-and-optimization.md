# Performance & Optimization

* Multi-threading for model inference
* Memory and GPU utilization
* Optimizing prediction latency
